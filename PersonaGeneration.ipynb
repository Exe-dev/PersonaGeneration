{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "173319109fbdc3b9e6669d3e6b9b4c0d8de214a81d3d8a11fbee43e8362bce4b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Rules of persona\r\n",
    "+ Each sentence must contain between 4 and 20 words or punctuation marks.\r\n",
    "+ It contains either the word I or my.\r\n",
    "+ At least one verb, and (iv) at least one noun, pronoun or adjective."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example Dialogue with persona\r\n",
    "- Persona: [“I like sport”, “I work a lot”]\r\n",
    "- Context: “I love running.”\r\n",
    "- Response: “Me too! But only on weekends.”"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "{\r\n",
    "    \r\n",
    "    \"dialog\": [[\"没有 钱   万万 不行 ！ ~\"], [\"现实 就是 如此\"]], \r\n",
    "    \r\n",
    "    \"profile\": [{\"tag\": [\"漫画;旅遊;星座\"], \"loc\": \"广东 广州\", \"gender\": \"male\"}, {\"tag\": [\"\"], \"loc\": \"\", \"gender\": \"\"}], \r\n",
    "    \r\n",
    "    \"uid\": [0, 1]\r\n",
    "\r\n",
    "}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "{\r\n",
    "    \r\n",
    "    \"dialog\": [[\"For what it's worth, I don't have a problem with it.\"], [\"My apologies.  I did not have any problems with it, but I will be more careful in the future.\"]], \r\n",
    "    \r\n",
    "    \"profile\": [{\"tag\": [], \"loc\": \"\", \"gender\": \"\"}, {\"tag\": [\"i did not have any problems with it, but i will be more careful in the future.\"], \r\n",
    "    \r\n",
    "    \"loc\": \"\", \"gender\": \"\"}], \r\n",
    "    \r\n",
    "    \"uid\": [0, 1]\r\n",
    "\r\n",
    "}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# test_data Example\r\n",
    "{\"uid\": [0, 1, 2], \"dialog\": [[\"剧烈运动 是 吧\"], [\"各种 剧烈运动\"], [\"... 姐 最近 有点 寂寞 过头 了 ...\"]], \"responder_profile\": {\"loc\": \"海南\", \"gender\": \"female\", \"tag\": \"美食;宅;80后\"}, \"profile\": [{\"loc\": \"天津 滨海新区\", \"gender\": \"male\", \"tag\": \"\"}, {\"loc\": \"海南\", \"gender\": \"female\", \"tag\": \"美食;宅;80后\"}, {\"loc\": \"安徽 合肥\", \"gender\": \"male\", \"tag\": \"游戏动漫;双子座;宅;音乐;90后;WOW台服众\"}], \"golden_response\": [\"可不是 ， 我 又 不 像 你 ， 有 女神 。\"]}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "import argparse\r\n",
    "import sys\r\n",
    "import pandas as pd \r\n",
    "import json\r\n",
    "import bz2\r\n",
    "from tqdm import tqdm\r\n",
    "import glob\r\n",
    "import dask.dataframe as dd\r\n",
    "from dask.diagnostics import ProgressBar\r\n",
    "import spacy\r\n",
    "import os\r\n",
    "import redditcleaner\r\n",
    "import neuralcoref"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cudf'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-1b40257775a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcudf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cudf'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Command Parser"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parser = argparse.ArgumentParser(description=\"preprocess of train data\")\r\n",
    "parser.add_argument(\"--npartitions\", dest=\"npartitions\", type=int, default=10,help=\"Number of partitions\")\r\n",
    "parser.add_argument(\"--input_json\", dest=\"input_json\", type=str, default=\"./reddit_data/*/*.json\" ,help=\"Input json path\")\r\n",
    "parser.add_argument(\"--output_path\", dest=\"output_path\", type=str, default=\"./outputs\" ,help=\"Output file path\")\r\n",
    "parser.add_argument(\"--scheduler\", dest=\"scheduler\", type=str, default=\"threads\" ,help=\"Selecting Threads, Processes, or Single Threaded\")\r\n",
    "parser.add_argument(\"--is_gpu\", dest=\"is_gpu\", type=bool, default=False ,help=\"If you want to use gpu for processing dataframe, you set True\")\r\n",
    "if \"ipykernel\" in sys.modules:\r\n",
    "    args = parser.parse_args(args=[])\r\n",
    "else:\r\n",
    "    args = parser.parse_args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constant"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NPARTITIONS = args.npartitions\r\n",
    "INPUT_JSON = args.input_json\r\n",
    "OUTPUT_PATH = args.output_path\r\n",
    "SCHEDULER = args.scheduler\r\n",
    "IS_GPU = args.is_gpu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tqdm.pandas()\r\n",
    "ProgressBar().register()\r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "neuralcoref.add_to_pipe(nlp)\r\n",
    "\r\n",
    "#doc1 = nlp('My sister has a dog. She loves him.')\r\n",
    "#print(doc1._.coref_resolved)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if(not os.path.exists(OUTPUT_PATH)):\r\n",
    "    os.makedirs(OUTPUT_PATH)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "version = len([f for f in os.listdir(OUTPUT_PATH) if \"persona\" in f])\r\n",
    "version"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# reddit_data下の全てのjsonファイルを読み込む"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list_bz2_file = glob.glob(INPUT_JSON)\r\n",
    "list_reddit_conversation = []\r\n",
    "list_bz2_file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"----------read input json files----------\")\r\n",
    "for i in tqdm(range(0,len(list_bz2_file))):\r\n",
    "    with open(list_bz2_file[i], mode=\"r\", encoding=\"utf-8\") as f:\r\n",
    "        for line in f.readlines():\r\n",
    "            dic=json.loads(line)\r\n",
    "            list_reddit_conversation.append(dic)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if IS_GPU:\r\n",
    "    import cudf\r\n",
    "    df_reddit_conversation = cudf.DataFrame(list_reddit_conversation)\r\n",
    "else:\r\n",
    "    df_reddit_conversation = pd.DataFrame(list_reddit_conversation)\r\n",
    "\r\n",
    "df_reddit_conversation = df_reddit_conversation[df_reddit_conversation[\"author\"]!=\"[deleted]\"]\r\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].progress_map(lambda x:redditcleaner.clean(str(x)))\r\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].replace([\"&lt\",\"&gt\",\"&amp\"],[\"\",\"\",\"\"])\r\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].replace([\"\\\\n+\",\"\\\\r\",\"\\\\\\\\\",\"”\",\"’\"],[\"\",\"\",\"\",\"\",\"\"], regex=True)\r\n",
    "df_reddit_conversation.to_csv(f\"{OUTPUT_PATH}/AllConversation{version}.csv\")\r\n",
    "df_reddit_conversation.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_reddit_conversation[df_reddit_conversation[\"body\"].str.contains(\"There's something called an\")]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(df_reddit_conversation[df_reddit_conversation[\"id\"]==\"c029e0e\"][\"body\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 会話ペアの作成"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_reddit_conversation[\"removed_prefix_parent_id\"] = df_reddit_conversation[\"parent_id\"].str.replace(\"t\\d_\",\"\")\r\n",
    "df_reddit_conversation[\"parent_body\"] = df_reddit_conversation[df_reddit_conversation[\"removed_prefix_parent_id\"]==df_reddit_conversation[\"id\"]][\"body\"]\r\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].str.replace('\\\"','’')\r\n",
    "df_reddit_conversation[\"parent_body\"] = df_reddit_conversation[\"parent_body\"].str.replace('\\\"','’')\r\n",
    "df_reddit_conversation = pd.merge(df_reddit_conversation,df_reddit_conversation[[\"id\",\"body\"]].rename(columns={\"id\":\"parent_id\",\"body\":\"parent_body\"}),left_on=\"removed_prefix_parent_id\",right_on=\"parent_id\").drop(columns=[\"parent_body_x\",\"parent_id_y\"]).rename(columns={\"parent_body_y\":\"parent_body\"})\r\n",
    "df_reddit_conversation = df_reddit_conversation.dropna(subset=[\"parent_body\"]).sort_values([\"author\"]).reset_index(drop=True)\r\n",
    "df_reddit_conversation[\"original_body\"] = df_reddit_conversation[\"body\"]\r\n",
    "df_reddit_conversation[\"original_parent_body\"] = df_reddit_conversation[\"parent_body\"]\r\n",
    "df_reddit_conversation = df_reddit_conversation[[\"body\",\"parent_body\",\"original_body\",\"original_parent_body\",\"ups\",\"author\"]]\r\n",
    "df_reddit_conversation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def CreatePersona(body: str):\r\n",
    "    doc = nlp(body.lower())\r\n",
    "    # 文ごとに分割\r\n",
    "    persona = [str(sentence) for sentence in doc.sents if IsPersona(str(sentence))]\r\n",
    "    return persona"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def IsPersona(sentence: str):\r\n",
    "    # 以下の3つの条件を満たすものをペルソナとする\r\n",
    "    # 1.文の単語数が4-20の間\r\n",
    "    # 2.I か my　が含まれている\r\n",
    "    # 3.少なくとも1つの動詞と，名詞，代名詞，形容詞のいずれかが含まれている\r\n",
    "    words = [str(word) for word in nlp(sentence.strip())]\r\n",
    "    poses = [token.pos_ for token in nlp(sentence.strip())]\r\n",
    "    return (\r\n",
    "        (4 <= len(words) <= 20)&\r\n",
    "        (not set([\"i\",\"my\"]).isdisjoint(set(words)))&\r\n",
    "        ((\"VERB\" in poses)&(not set([\"NOUN\", \"ADJ\", \"PROPN\"]).isdisjoint(set(poses))))\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_json(row):\r\n",
    "    return {\r\n",
    "        \"dialog\":row[\"dialog\"],\r\n",
    "        \"profile\":[\r\n",
    "            {\"tag\":row[\"persona\"],\r\n",
    "            \"loc\":\"\",\r\n",
    "            \"gender\":\"\"},\r\n",
    "            {\"tag\":row[\"parent_persona\"],\r\n",
    "            \"loc\":\"\",\r\n",
    "            \"gender\":\"\"}\r\n",
    "        ],\r\n",
    "        \"uid\":[0,1]\r\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ペルソナの作成"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"----------create conversation pair ----------\")\r\n",
    "ddf_reddit_conversation = dd.from_pandas(data=df_reddit_conversation, npartitions=NPARTITIONS)\r\n",
    "ddf_reddit_conversation[\"persona\"] = ddf_reddit_conversation[\"original_body\"].map(CreatePersona)\r\n",
    "ddf_reddit_conversation[\"parent_persona\"] = ddf_reddit_conversation[\"original_parent_body\"].map(CreatePersona)\r\n",
    "ddf_reddit_conversation.query(\"persona.notnull() | parent_persona.notnull()\")\r\n",
    "#ddf_reddit_conversation[\"body\"] = ddf_reddit_conversation[\"body\"].map(lambda sentence:nlp(sentence)._.coref_resolved)\r\n",
    "#ddf_reddit_conversation[\"parent_body\"] = ddf_reddit_conversation[\"parent_body\"].map(lambda sentence:nlp(sentence)._.coref_resolved)\r\n",
    "df_reddit_conversation = ddf_reddit_conversation.compute(scheduler=SCHEDULER)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"--------- create list ----------\")\r\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].progress_map(lambda x: [x] )\r\n",
    "df_reddit_conversation[\"parent_body\"] = df_reddit_conversation[\"parent_body\"].progress_map(lambda x: [x] )\r\n",
    "df_reddit_conversation[\"dialog\"] = [list(x) for x in zip(df_reddit_conversation[\"body\"].tolist(),df_reddit_conversation[\"parent_body\"].tolist())]\r\n",
    "df_reddit_conversation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Json形式の作成"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_reddit_conversation[\"json\"] = df_reddit_conversation.progress_apply(create_json, axis=1)\r\n",
    "df_reddit_conversation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Outputs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_reddit_conversation.to_csv(f\"{OUTPUT_PATH}/persona{version}.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list_json = df_reddit_conversation[\"json\"].tolist()\r\n",
    "with open(f\"{OUTPUT_PATH}/created_dialogues{version}.json\", \"wt\", encoding=\"utf-8\") as file:\r\n",
    "    for dic in list_json:\r\n",
    "        file.write(str(json.dumps(dic))+\"\\n\")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "import subprocess\r\n",
    "subprocess.run(['jupyter', 'nbconvert', '--to', 'script', '*.ipynb'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CompletedProcess(args=['jupyter', 'nbconvert', '--to', 'script', '*.ipynb'], returncode=0)"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {}
  }
 ]
}