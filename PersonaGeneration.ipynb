{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "173319109fbdc3b9e6669d3e6b9b4c0d8de214a81d3d8a11fbee43e8362bce4b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Rules of persona\n",
    "+ Each sentence must contain between 4 and 20 words or punctuation marks.\n",
    "+ It contains either the word I or my.\n",
    "+ At least one verb, and (iv) at least one noun, pronoun or adjective."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Example Dialogue with persona\n",
    "- Persona: [“I like sport”, “I work a lot”]\n",
    "- Context: “I love running.”\n",
    "- Response: “Me too! But only on weekends.”"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import bz2\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import spacy\n",
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# reddit_data下の全てのjsonファイルを読み込む"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./reddit_data\\\\2007\\\\RC_2007-10.json',\n",
       " './reddit_data\\\\2008\\\\RC_2008-01.json']"
      ]
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "list_bz2_file = glob.glob(\"./reddit_data/*/*.json\")\n",
    "list_reddit_conversation = []\n",
    "list_bz2_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(list_bz2_file)):\n",
    "    with open(list_bz2_file[i]) as f:\n",
    "        for line in f.readlines():\n",
    "            dic=json.loads(line)\n",
    "            list_reddit_conversation.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        parent_id created_utc  ups  controversiality distinguished  \\\n",
       "0        t3_5yba3  1192450635    1                 0          None   \n",
       "1        t3_5yba3  1192450639    2                 0          None   \n",
       "2      t1_c02999p  1192450643    0                 0          None   \n",
       "3        t3_5yba3  1192450646    1                 0          None   \n",
       "4      t1_c0299ah  1192450646    3                 0          None   \n",
       "...           ...         ...  ...               ...           ...   \n",
       "18509  t1_c02scza  1199201443    1                 0          None   \n",
       "18510    t3_649dw  1199201450   -4                 0          None   \n",
       "18511    t3_649ct  1199201461   -1                 0          None   \n",
       "18512  t1_c02sddc  1199201480    1                 0          None   \n",
       "18513  t1_c02sbud  1199201487    1                 0          None   \n",
       "\n",
       "      subreddit_id       id  downs  archived   link_id  ...          author  \\\n",
       "0             t5_6  c0299an      0      True  t3_5yba3  ...         bostich   \n",
       "1             t5_6  c0299ao      0      True  t3_5yba3  ...  igiveyoumylife   \n",
       "2             t5_6  c0299ap      0      True  t3_5yba3  ...            Arve   \n",
       "3             t5_6  c0299aq      0      True  t3_5yba3  ...       [deleted]   \n",
       "4             t5_6  c0299ar      0      True  t3_5yba3  ...       gigaquack   \n",
       "...            ...      ...    ...       ...       ...  ...             ...   \n",
       "18509     t5_2cneq  c02sdem      0      True  t3_6483n  ...     DogBotherer   \n",
       "18510     t5_2cneq  c02sden      0      True  t3_649dw  ...        theDrWho   \n",
       "18511         t5_6  c02sdeo      0      True  t3_649ct  ...      evilwombat   \n",
       "18512         t5_6  c02sdep      0      True  t3_649h8  ...       otterplay   \n",
       "18513     t5_2cneq  c02sdeq      0      True  t3_648qz  ...      elissa1959   \n",
       "\n",
       "      score_hidden                                               body gilded  \\\n",
       "0            False                                               test      0   \n",
       "1            False  much smoother.\\r\\n\\r\\nIm just glad reddit is b...      0   \n",
       "2            False  Can we please deprecate the word \"Ajax\" now? \\...      0   \n",
       "3            False                                          [deleted]      0   \n",
       "4            False         Oh, I see. Fancy schmancy \"submitting....\"      0   \n",
       "...            ...                                                ...    ...   \n",
       "18509        False  Maybe I've just been away too long, but I can'...      0   \n",
       "18510        False                                  religion of peace      0   \n",
       "18511        False  The problem is that too often, we end up payin...      0   \n",
       "18512        False              Our president is a dualist, actually.      0   \n",
       "18513        False                                Speak for yourself.      0   \n",
       "\n",
       "       author_flair_text   subreddit edited author_flair_css_class  \\\n",
       "0                   None  reddit.com  False                   None   \n",
       "1                   None  reddit.com  False                   None   \n",
       "2                   None  reddit.com  False                   None   \n",
       "3                   None  reddit.com  False                   None   \n",
       "4                   None  reddit.com  False                   None   \n",
       "...                  ...         ...    ...                    ...   \n",
       "18509               None    politics  False                   None   \n",
       "18510               None    politics  False                   None   \n",
       "18511               None  reddit.com   True                   None   \n",
       "18512               None  reddit.com  False                   None   \n",
       "18513               None    politics  False                   None   \n",
       "\n",
       "             name retrieved_on  \n",
       "0      t1_c0299an   1427426409  \n",
       "1      t1_c0299ao   1427426409  \n",
       "2      t1_c0299ap   1427426409  \n",
       "3      t1_c0299aq   1427426409  \n",
       "4      t1_c0299ar   1427426409  \n",
       "...           ...          ...  \n",
       "18509  t1_c02sdem   1425820211  \n",
       "18510  t1_c02sden   1425820211  \n",
       "18511  t1_c02sdeo   1425820211  \n",
       "18512  t1_c02sdep   1425820211  \n",
       "18513  t1_c02sdeq   1425820211  \n",
       "\n",
       "[18514 rows x 21 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>parent_id</th>\n      <th>created_utc</th>\n      <th>ups</th>\n      <th>controversiality</th>\n      <th>distinguished</th>\n      <th>subreddit_id</th>\n      <th>id</th>\n      <th>downs</th>\n      <th>archived</th>\n      <th>link_id</th>\n      <th>...</th>\n      <th>author</th>\n      <th>score_hidden</th>\n      <th>body</th>\n      <th>gilded</th>\n      <th>author_flair_text</th>\n      <th>subreddit</th>\n      <th>edited</th>\n      <th>author_flair_css_class</th>\n      <th>name</th>\n      <th>retrieved_on</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t3_5yba3</td>\n      <td>1192450635</td>\n      <td>1</td>\n      <td>0</td>\n      <td>None</td>\n      <td>t5_6</td>\n      <td>c0299an</td>\n      <td>0</td>\n      <td>True</td>\n      <td>t3_5yba3</td>\n      <td>...</td>\n      <td>bostich</td>\n      <td>False</td>\n      <td>test</td>\n      <td>0</td>\n      <td>None</td>\n      <td>reddit.com</td>\n      <td>False</td>\n      <td>None</td>\n      <td>t1_c0299an</td>\n      <td>1427426409</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t3_5yba3</td>\n      <td>1192450639</td>\n      <td>2</td>\n      <td>0</td>\n      <td>None</td>\n      <td>t5_6</td>\n      <td>c0299ao</td>\n      <td>0</td>\n      <td>True</td>\n      <td>t3_5yba3</td>\n      <td>...</td>\n      <td>igiveyoumylife</td>\n      <td>False</td>\n      <td>much smoother.\\r\\n\\r\\nIm just glad reddit is b...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>reddit.com</td>\n      <td>False</td>\n      <td>None</td>\n      <td>t1_c0299ao</td>\n      <td>1427426409</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t1_c02999p</td>\n      <td>1192450643</td>\n      <td>0</td>\n      <td>0</td>\n      <td>None</td>\n      <td>t5_6</td>\n      <td>c0299ap</td>\n      <td>0</td>\n      <td>True</td>\n      <td>t3_5yba3</td>\n      <td>...</td>\n      <td>Arve</td>\n      <td>False</td>\n      <td>Can we please deprecate the word \"Ajax\" now? \\...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>reddit.com</td>\n      <td>False</td>\n      <td>None</td>\n      <td>t1_c0299ap</td>\n      <td>1427426409</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t3_5yba3</td>\n      <td>1192450646</td>\n      <td>1</td>\n      <td>0</td>\n      <td>None</td>\n      <td>t5_6</td>\n      <td>c0299aq</td>\n      <td>0</td>\n      <td>True</td>\n      <td>t3_5yba3</td>\n      <td>...</td>\n      <td>[deleted]</td>\n      <td>False</td>\n      <td>[deleted]</td>\n      <td>0</td>\n      <td>None</td>\n      <td>reddit.com</td>\n      <td>False</td>\n      <td>None</td>\n      <td>t1_c0299aq</td>\n      <td>1427426409</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t1_c0299ah</td>\n      <td>1192450646</td>\n      <td>3</td>\n      <td>0</td>\n      <td>None</td>\n      <td>t5_6</td>\n      <td>c0299ar</td>\n      <td>0</td>\n      <td>True</td>\n      <td>t3_5yba3</td>\n      <td>...</td>\n      <td>gigaquack</td>\n      <td>False</td>\n      <td>Oh, I see. Fancy schmancy \"submitting....\"</td>\n      <td>0</td>\n      <td>None</td>\n      <td>reddit.com</td>\n      <td>False</td>\n      <td>None</td>\n      <td>t1_c0299ar</td>\n      <td>1427426409</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18509</th>\n      <td>t1_c02scza</td>\n      <td>1199201443</td>\n      <td>1</td>\n      <td>0</td>\n      <td>None</td>\n      <td>t5_2cneq</td>\n      <td>c02sdem</td>\n      <td>0</td>\n      <td>True</td>\n      <td>t3_6483n</td>\n      <td>...</td>\n      <td>DogBotherer</td>\n      <td>False</td>\n      <td>Maybe I've just been away too long, but I can'...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>politics</td>\n      <td>False</td>\n      <td>None</td>\n      <td>t1_c02sdem</td>\n      <td>1425820211</td>\n    </tr>\n    <tr>\n      <th>18510</th>\n      <td>t3_649dw</td>\n      <td>1199201450</td>\n      <td>-4</td>\n      <td>0</td>\n      <td>None</td>\n      <td>t5_2cneq</td>\n      <td>c02sden</td>\n      <td>0</td>\n      <td>True</td>\n      <td>t3_649dw</td>\n      <td>...</td>\n      <td>theDrWho</td>\n      <td>False</td>\n      <td>religion of peace</td>\n      <td>0</td>\n      <td>None</td>\n      <td>politics</td>\n      <td>False</td>\n      <td>None</td>\n      <td>t1_c02sden</td>\n      <td>1425820211</td>\n    </tr>\n    <tr>\n      <th>18511</th>\n      <td>t3_649ct</td>\n      <td>1199201461</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>None</td>\n      <td>t5_6</td>\n      <td>c02sdeo</td>\n      <td>0</td>\n      <td>True</td>\n      <td>t3_649ct</td>\n      <td>...</td>\n      <td>evilwombat</td>\n      <td>False</td>\n      <td>The problem is that too often, we end up payin...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>reddit.com</td>\n      <td>True</td>\n      <td>None</td>\n      <td>t1_c02sdeo</td>\n      <td>1425820211</td>\n    </tr>\n    <tr>\n      <th>18512</th>\n      <td>t1_c02sddc</td>\n      <td>1199201480</td>\n      <td>1</td>\n      <td>0</td>\n      <td>None</td>\n      <td>t5_6</td>\n      <td>c02sdep</td>\n      <td>0</td>\n      <td>True</td>\n      <td>t3_649h8</td>\n      <td>...</td>\n      <td>otterplay</td>\n      <td>False</td>\n      <td>Our president is a dualist, actually.</td>\n      <td>0</td>\n      <td>None</td>\n      <td>reddit.com</td>\n      <td>False</td>\n      <td>None</td>\n      <td>t1_c02sdep</td>\n      <td>1425820211</td>\n    </tr>\n    <tr>\n      <th>18513</th>\n      <td>t1_c02sbud</td>\n      <td>1199201487</td>\n      <td>1</td>\n      <td>0</td>\n      <td>None</td>\n      <td>t5_2cneq</td>\n      <td>c02sdeq</td>\n      <td>0</td>\n      <td>True</td>\n      <td>t3_648qz</td>\n      <td>...</td>\n      <td>elissa1959</td>\n      <td>False</td>\n      <td>Speak for yourself.</td>\n      <td>0</td>\n      <td>None</td>\n      <td>politics</td>\n      <td>False</td>\n      <td>None</td>\n      <td>t1_c02sdeq</td>\n      <td>1425820211</td>\n    </tr>\n  </tbody>\n</table>\n<p>18514 rows × 21 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 179
    }
   ],
   "source": [
    "df_reddit_conversation = pd.DataFrame(list_reddit_conversation)\n",
    "df_reddit_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'neuralcoref' has no attribute 'add_to_pipe'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-250-7a6a957b5412>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mneuralcoref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdoc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'My sister has a dog. She loves him.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoref_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'neuralcoref' has no attribute 'add_to_pipe'"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "doc1 = nlp('My sister has a dog. She loves him.')\n",
    "print(doc1._.coref_clusters)\n",
    "\n",
    "doc2 = nlp('Angela lives in Boston. She is quite happy in that city.')\n",
    "for ent in doc2.ents:\n",
    "    print(ent._.coref_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-220-5557c5a47ea6>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n  df_reddit_conversation[\"removed_prefix_parent_id\"] = df_reddit_conversation[\"parent_id\"].str.replace(\"t\\d_\",\"\")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   body  \\\n",
       "0     For what it's worth, I don't have a problem wi...   \n",
       "1                 Like hormonal imbalances and cancer?    \n",
       "2     Yeah I wish I could get the test some attentio...   \n",
       "3                                     Happy New Year :)   \n",
       "4     Hmm, I came out as ’centrist’ but right betwee...   \n",
       "...                                                 ...   \n",
       "7843  Thanks for that- I always assumed the kinetic ...   \n",
       "7844  http://www.fff.org/comment/com0311c.asp\\n\\nmay...   \n",
       "7845  Yeah ok, but what I do is just go rapidly over...   \n",
       "7846  ’social’ added to ’problems’ do not have the s...   \n",
       "7847  [Frontline: Secret history of the credit card]...   \n",
       "\n",
       "                                            parent_body  ups      author  \n",
       "0     My apologies.  I did not have any problems wit...    2  0gleth0rpe  \n",
       "1     Has anyone ever considered that perhaps the pl...    1    24sparky  \n",
       "2     I took that 6 page survey and ended up where i...    1    24sparky  \n",
       "3     Thanks for the links, im looking through them ...    1    24sparky  \n",
       "4     World's Smallest Political Quiz:\\r\\nhttp://www...    1    24sparky  \n",
       "...                                                 ...  ...         ...  \n",
       "7843  Not that it changes your point, but I think yo...    4  zoomzoom83  \n",
       "7844  Wow! What a shocker! After Saddam attempted to...    1       zorno  \n",
       "7845  I like it because the old system lent itself t...    1     zouhair  \n",
       "7846  Oh okay, you're right, let's just cancel all s...    2     zouhair  \n",
       "7847  Better than Maxed Out (which was ok), and on t...    2        zxvf  \n",
       "\n",
       "[7848 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>body</th>\n      <th>parent_body</th>\n      <th>ups</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>For what it's worth, I don't have a problem wi...</td>\n      <td>My apologies.  I did not have any problems wit...</td>\n      <td>2</td>\n      <td>0gleth0rpe</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Like hormonal imbalances and cancer?</td>\n      <td>Has anyone ever considered that perhaps the pl...</td>\n      <td>1</td>\n      <td>24sparky</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Yeah I wish I could get the test some attentio...</td>\n      <td>I took that 6 page survey and ended up where i...</td>\n      <td>1</td>\n      <td>24sparky</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Happy New Year :)</td>\n      <td>Thanks for the links, im looking through them ...</td>\n      <td>1</td>\n      <td>24sparky</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hmm, I came out as ’centrist’ but right betwee...</td>\n      <td>World's Smallest Political Quiz:\\r\\nhttp://www...</td>\n      <td>1</td>\n      <td>24sparky</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7843</th>\n      <td>Thanks for that- I always assumed the kinetic ...</td>\n      <td>Not that it changes your point, but I think yo...</td>\n      <td>4</td>\n      <td>zoomzoom83</td>\n    </tr>\n    <tr>\n      <th>7844</th>\n      <td>http://www.fff.org/comment/com0311c.asp\\n\\nmay...</td>\n      <td>Wow! What a shocker! After Saddam attempted to...</td>\n      <td>1</td>\n      <td>zorno</td>\n    </tr>\n    <tr>\n      <th>7845</th>\n      <td>Yeah ok, but what I do is just go rapidly over...</td>\n      <td>I like it because the old system lent itself t...</td>\n      <td>1</td>\n      <td>zouhair</td>\n    </tr>\n    <tr>\n      <th>7846</th>\n      <td>’social’ added to ’problems’ do not have the s...</td>\n      <td>Oh okay, you're right, let's just cancel all s...</td>\n      <td>2</td>\n      <td>zouhair</td>\n    </tr>\n    <tr>\n      <th>7847</th>\n      <td>[Frontline: Secret history of the credit card]...</td>\n      <td>Better than Maxed Out (which was ok), and on t...</td>\n      <td>2</td>\n      <td>zxvf</td>\n    </tr>\n  </tbody>\n</table>\n<p>7848 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 220
    }
   ],
   "source": [
    "df_reddit_conversation = pd.DataFrame(list_reddit_conversation)\n",
    "df_reddit_conversation = df_reddit_conversation[df_reddit_conversation[\"body\"]!=\"[deleted]\"]\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].replace([\"&lt\",\"&gt\",\"&amp\"],[\"\",\"\",\"\"])\n",
    "df_reddit_conversation[\"removed_prefix_parent_id\"] = df_reddit_conversation[\"parent_id\"].str.replace(\"t\\d_\",\"\")\n",
    "df_reddit_conversation[\"parent_body\"] = df_reddit_conversation[df_reddit_conversation[\"removed_prefix_parent_id\"]==df_reddit_conversation[\"id\"]][\"body\"]\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].str.replace('\\\"','’')\n",
    "df_reddit_conversation[\"parent_body\"] = df_reddit_conversation[\"parent_body\"].str.replace('\\\"','’')\n",
    "df_reddit_conversation = pd.merge(df_reddit_conversation,df_reddit_conversation[[\"id\",\"body\"]].rename(columns={\"id\":\"parent_id\",\"body\":\"parent_body\"}),left_on=\"removed_prefix_parent_id\",right_on=\"parent_id\").drop(columns=[\"parent_body_x\",\"parent_id_y\"]).rename(columns={\"parent_body_y\":\"parent_body\"})\n",
    "df_reddit_conversation = df_reddit_conversation.dropna(subset=[\"parent_body\"]).sort_values([\"author\"]).reset_index(drop=True)\n",
    "df_reddit_conversation = df_reddit_conversation[[\"body\",\"parent_body\",\"ups\",\"author\"]]\n",
    "df_reddit_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreatePersona(body: str):\n",
    "    doc = nlp(body.lower())\n",
    "    # 文ごとに分割\n",
    "    persona = [str(sentence) for sentence in doc.sents if IsPersona(str(sentence))]\n",
    "    return persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsPersona(sentence: str):\n",
    "    # 以下の3つの条件を満たすものをペルソナとする\n",
    "    # 1.文の単語数が4-20の間\n",
    "    # 2.I か my　が含まれている\n",
    "    # 3.少なくとも1つの動詞と，名詞，代名詞，形容詞のいずれかが含まれている\n",
    "    words = [str(word) for word in nlp(sentence.strip())]\n",
    "    poses = [token.pos_ for token in nlp(sentence.strip())]\n",
    "    return (\n",
    "        (4 <= len(words) <= 20)&\n",
    "        (not set([\"i\",\"my\"]).isdisjoint(set(words)))&\n",
    "        ((\"VERB\" in poses)&(not set([\"NOUN\", \"ADJ\", \"PROPN\"]).isdisjoint(set(poses))))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(dialog, persona0, persona1):\n",
    "    return {\n",
    "        \"dialog\":dialog,\n",
    "        \"profile\":[\n",
    "            {\"tag\":persona0,\n",
    "            \"loc\":\"\",\n",
    "            \"gender\":\"\"},\n",
    "            {\"tag\":persona1,\n",
    "            \"loc\":\"\",\n",
    "            \"gender\":\"\"}\n",
    "        ],\n",
    "        \"uid\":[0,1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 18514/18514 [00:00<00:00, 925491.26it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                                                   [test]\n",
       "1        [much smoother.\\r\\n\\r\\nIm just glad reddit is ...\n",
       "2        [Can we please deprecate the word \"Ajax\" now? ...\n",
       "3                                              [[deleted]]\n",
       "4             [Oh, I see. Fancy schmancy \"submitting....\"]\n",
       "                               ...                        \n",
       "18509    [Maybe I've just been away too long, but I can...\n",
       "18510                                  [religion of peace]\n",
       "18511    [The problem is that too often, we end up payi...\n",
       "18512              [Our president is a dualist, actually.]\n",
       "18513                                [Speak for yourself.]\n",
       "Name: body, Length: 18514, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 7848/7848 [04:11<00:00, 31.17it/s]\n",
      "100%|██████████| 7848/7848 [04:51<00:00, 26.93it/s]\n",
      "100%|██████████| 3865/3865 [00:00<00:00, 772982.31it/s]\n",
      "100%|██████████| 3865/3865 [00:00<00:00, 772945.45it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   body  \\\n",
       "0     [For what it's worth, I don't have a problem w...   \n",
       "1     [Yeah I wish I could get the test some attenti...   \n",
       "2                                   [Happy New Year :)]   \n",
       "3     [Hmm, I came out as ’centrist’ but right betwe...   \n",
       "4     [I wonder if it's more than an attitude toward...   \n",
       "...                                                 ...   \n",
       "3860  [Well in fact I had to flush the browser cache...   \n",
       "3861  [Yes, but in typical reddit style anything I p...   \n",
       "3862  [Nope. Months of development, a day of downtim...   \n",
       "3863  [Thanks for that- I always assumed the kinetic...   \n",
       "3864  [Yeah ok, but what I do is just go rapidly ove...   \n",
       "\n",
       "                                            parent_body  ups      author  \\\n",
       "0     [My apologies.  I did not have any problems wi...    2  0gleth0rpe   \n",
       "1     [I took that 6 page survey and ended up where ...    1    24sparky   \n",
       "2     [Thanks for the links, im looking through them...    1    24sparky   \n",
       "3     [World's Smallest Political Quiz:\\r\\nhttp://ww...    1    24sparky   \n",
       "4     [The problem is not guns, it's attitude toward...    8    24sparky   \n",
       "...                                                 ...  ...         ...   \n",
       "3860                                [Works fine for me]    1      zmonk2   \n",
       "3861  [What's disturbing about it?  Think it's one o...    1  zoomzoom83   \n",
       "3862  [ Is anything else different? Surely there's m...    4  zoomzoom83   \n",
       "3863  [Not that it changes your point, but I think y...    4  zoomzoom83   \n",
       "3864  [I like it because the old system lent itself ...    1     zouhair   \n",
       "\n",
       "                                                persona  \\\n",
       "0     [for what it's worth, i don't have a problem w...   \n",
       "1                                                    []   \n",
       "2                                                    []   \n",
       "3     [hmm, i came out as ’centrist’ but right betwe...   \n",
       "4     [i wonder if it's more than an attitude toward...   \n",
       "...                                                 ...   \n",
       "3860  [well in fact i had to flush the browser cache...   \n",
       "3861           [i have to complete with ron paul here.]   \n",
       "3862  [\\n\\n(in reality, i think it's a complete rewr...   \n",
       "3863                                                 []   \n",
       "3864  [and then i scroll rapidly to another thread, ...   \n",
       "\n",
       "                                         parent_persona  \\\n",
       "0     [ i did not have any problems with it, but i w...   \n",
       "1     [i took that 6 page survey and ended up where ...   \n",
       "2     [thanks for the links, im looking through them...   \n",
       "3                                                    []   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "3860                                                 []   \n",
       "3861  [ think it's one of the funniest cartoons i've...   \n",
       "3862     [\\n\\nedit: i guess that sounded kinda snotty.]   \n",
       "3863  [not that it changes your point, but i think y...   \n",
       "3864                                                 []   \n",
       "\n",
       "                                                 dialog  \n",
       "0     [[For what it's worth, I don't have a problem ...  \n",
       "1     [[Yeah I wish I could get the test some attent...  \n",
       "2     [[Happy New Year :)], [Thanks for the links, i...  \n",
       "3     [[Hmm, I came out as ’centrist’ but right betw...  \n",
       "4     [[I wonder if it's more than an attitude towar...  \n",
       "...                                                 ...  \n",
       "3860  [[Well in fact I had to flush the browser cach...  \n",
       "3861  [[Yes, but in typical reddit style anything I ...  \n",
       "3862  [[Nope. Months of development, a day of downti...  \n",
       "3863  [[Thanks for that- I always assumed the kineti...  \n",
       "3864  [[Yeah ok, but what I do is just go rapidly ov...  \n",
       "\n",
       "[3865 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>body</th>\n      <th>parent_body</th>\n      <th>ups</th>\n      <th>author</th>\n      <th>persona</th>\n      <th>parent_persona</th>\n      <th>dialog</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[For what it's worth, I don't have a problem w...</td>\n      <td>[My apologies.  I did not have any problems wi...</td>\n      <td>2</td>\n      <td>0gleth0rpe</td>\n      <td>[for what it's worth, i don't have a problem w...</td>\n      <td>[ i did not have any problems with it, but i w...</td>\n      <td>[[For what it's worth, I don't have a problem ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[Yeah I wish I could get the test some attenti...</td>\n      <td>[I took that 6 page survey and ended up where ...</td>\n      <td>1</td>\n      <td>24sparky</td>\n      <td>[]</td>\n      <td>[i took that 6 page survey and ended up where ...</td>\n      <td>[[Yeah I wish I could get the test some attent...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Happy New Year :)]</td>\n      <td>[Thanks for the links, im looking through them...</td>\n      <td>1</td>\n      <td>24sparky</td>\n      <td>[]</td>\n      <td>[thanks for the links, im looking through them...</td>\n      <td>[[Happy New Year :)], [Thanks for the links, i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[Hmm, I came out as ’centrist’ but right betwe...</td>\n      <td>[World's Smallest Political Quiz:\\r\\nhttp://ww...</td>\n      <td>1</td>\n      <td>24sparky</td>\n      <td>[hmm, i came out as ’centrist’ but right betwe...</td>\n      <td>[]</td>\n      <td>[[Hmm, I came out as ’centrist’ but right betw...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[I wonder if it's more than an attitude toward...</td>\n      <td>[The problem is not guns, it's attitude toward...</td>\n      <td>8</td>\n      <td>24sparky</td>\n      <td>[i wonder if it's more than an attitude toward...</td>\n      <td>[]</td>\n      <td>[[I wonder if it's more than an attitude towar...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3860</th>\n      <td>[Well in fact I had to flush the browser cache...</td>\n      <td>[Works fine for me]</td>\n      <td>1</td>\n      <td>zmonk2</td>\n      <td>[well in fact i had to flush the browser cache...</td>\n      <td>[]</td>\n      <td>[[Well in fact I had to flush the browser cach...</td>\n    </tr>\n    <tr>\n      <th>3861</th>\n      <td>[Yes, but in typical reddit style anything I p...</td>\n      <td>[What's disturbing about it?  Think it's one o...</td>\n      <td>1</td>\n      <td>zoomzoom83</td>\n      <td>[i have to complete with ron paul here.]</td>\n      <td>[ think it's one of the funniest cartoons i've...</td>\n      <td>[[Yes, but in typical reddit style anything I ...</td>\n    </tr>\n    <tr>\n      <th>3862</th>\n      <td>[Nope. Months of development, a day of downtim...</td>\n      <td>[ Is anything else different? Surely there's m...</td>\n      <td>4</td>\n      <td>zoomzoom83</td>\n      <td>[\\n\\n(in reality, i think it's a complete rewr...</td>\n      <td>[\\n\\nedit: i guess that sounded kinda snotty.]</td>\n      <td>[[Nope. Months of development, a day of downti...</td>\n    </tr>\n    <tr>\n      <th>3863</th>\n      <td>[Thanks for that- I always assumed the kinetic...</td>\n      <td>[Not that it changes your point, but I think y...</td>\n      <td>4</td>\n      <td>zoomzoom83</td>\n      <td>[]</td>\n      <td>[not that it changes your point, but i think y...</td>\n      <td>[[Thanks for that- I always assumed the kineti...</td>\n    </tr>\n    <tr>\n      <th>3864</th>\n      <td>[Yeah ok, but what I do is just go rapidly ove...</td>\n      <td>[I like it because the old system lent itself ...</td>\n      <td>1</td>\n      <td>zouhair</td>\n      <td>[and then i scroll rapidly to another thread, ...</td>\n      <td>[]</td>\n      <td>[[Yeah ok, but what I do is just go rapidly ov...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3865 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "source": [
    "df_reddit_conversation[\"persona\"] = df_reddit_conversation[\"body\"].progress_apply(CreatePersona)\n",
    "df_reddit_conversation[\"parent_persona\"] = df_reddit_conversation[\"parent_body\"].progress_apply(CreatePersona)\n",
    "df_reddit_conversation = df_reddit_conversation[(df_reddit_conversation.astype(str)[\"persona\"] !=\"[]\")|(df_reddit_conversation.astype(str)[\"parent_persona\"] !=\"[]\")].reset_index(drop=True)\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].progress_apply(lambda x: [x] )\n",
    "df_reddit_conversation[\"parent_body\"] = df_reddit_conversation[\"parent_body\"].progress_apply(lambda x: [x] )\n",
    "df_reddit_conversation[\"dialog\"] = [list(x) for x in zip(df_reddit_conversation[\"body\"].tolist(),df_reddit_conversation[\"parent_body\"].tolist())]\n",
    "df_reddit_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "3865it [00:00, 8853.45it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   body  \\\n",
       "0     [For what it's worth, I don't have a problem w...   \n",
       "1     [Yeah I wish I could get the test some attenti...   \n",
       "2                                   [Happy New Year :)]   \n",
       "3     [Hmm, I came out as ’centrist’ but right betwe...   \n",
       "4     [I wonder if it's more than an attitude toward...   \n",
       "...                                                 ...   \n",
       "3860  [Well in fact I had to flush the browser cache...   \n",
       "3861  [Yes, but in typical reddit style anything I p...   \n",
       "3862  [Nope. Months of development, a day of downtim...   \n",
       "3863  [Thanks for that- I always assumed the kinetic...   \n",
       "3864  [Yeah ok, but what I do is just go rapidly ove...   \n",
       "\n",
       "                                            parent_body  ups      author  \\\n",
       "0     [My apologies.  I did not have any problems wi...    2  0gleth0rpe   \n",
       "1     [I took that 6 page survey and ended up where ...    1    24sparky   \n",
       "2     [Thanks for the links, im looking through them...    1    24sparky   \n",
       "3     [World's Smallest Political Quiz:\\r\\nhttp://ww...    1    24sparky   \n",
       "4     [The problem is not guns, it's attitude toward...    8    24sparky   \n",
       "...                                                 ...  ...         ...   \n",
       "3860                                [Works fine for me]    1      zmonk2   \n",
       "3861  [What's disturbing about it?  Think it's one o...    1  zoomzoom83   \n",
       "3862  [ Is anything else different? Surely there's m...    4  zoomzoom83   \n",
       "3863  [Not that it changes your point, but I think y...    4  zoomzoom83   \n",
       "3864  [I like it because the old system lent itself ...    1     zouhair   \n",
       "\n",
       "                                                persona  \\\n",
       "0     [for what it's worth, i don't have a problem w...   \n",
       "1                                                    []   \n",
       "2                                                    []   \n",
       "3     [hmm, i came out as ’centrist’ but right betwe...   \n",
       "4     [i wonder if it's more than an attitude toward...   \n",
       "...                                                 ...   \n",
       "3860  [well in fact i had to flush the browser cache...   \n",
       "3861           [i have to complete with ron paul here.]   \n",
       "3862  [\\n\\n(in reality, i think it's a complete rewr...   \n",
       "3863                                                 []   \n",
       "3864  [and then i scroll rapidly to another thread, ...   \n",
       "\n",
       "                                         parent_persona  \\\n",
       "0     [ i did not have any problems with it, but i w...   \n",
       "1     [i took that 6 page survey and ended up where ...   \n",
       "2     [thanks for the links, im looking through them...   \n",
       "3                                                    []   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "3860                                                 []   \n",
       "3861  [ think it's one of the funniest cartoons i've...   \n",
       "3862     [\\n\\nedit: i guess that sounded kinda snotty.]   \n",
       "3863  [not that it changes your point, but i think y...   \n",
       "3864                                                 []   \n",
       "\n",
       "                                                 dialog  \\\n",
       "0     [[For what it's worth, I don't have a problem ...   \n",
       "1     [[Yeah I wish I could get the test some attent...   \n",
       "2     [[Happy New Year :)], [Thanks for the links, i...   \n",
       "3     [[Hmm, I came out as ’centrist’ but right betw...   \n",
       "4     [[I wonder if it's more than an attitude towar...   \n",
       "...                                                 ...   \n",
       "3860  [[Well in fact I had to flush the browser cach...   \n",
       "3861  [[Yes, but in typical reddit style anything I ...   \n",
       "3862  [[Nope. Months of development, a day of downti...   \n",
       "3863  [[Thanks for that- I always assumed the kineti...   \n",
       "3864  [[Yeah ok, but what I do is just go rapidly ov...   \n",
       "\n",
       "                                                   json  \n",
       "0     {'dialog': [['For what it's worth, I don't hav...  \n",
       "1     {'dialog': [['Yeah I wish I could get the test...  \n",
       "2     {'dialog': [['Happy New Year :)'], ['Thanks fo...  \n",
       "3     {'dialog': [['Hmm, I came out as ’centrist’ bu...  \n",
       "4     {'dialog': [['I wonder if it's more than an at...  \n",
       "...                                                 ...  \n",
       "3860  {'dialog': [['Well in fact I had to flush the ...  \n",
       "3861  {'dialog': [['Yes, but in typical reddit style...  \n",
       "3862  {'dialog': [['Nope. Months of development, a d...  \n",
       "3863  {'dialog': [['Thanks for that- I always assume...  \n",
       "3864  {'dialog': [['Yeah ok, but what I do is just g...  \n",
       "\n",
       "[3865 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>body</th>\n      <th>parent_body</th>\n      <th>ups</th>\n      <th>author</th>\n      <th>persona</th>\n      <th>parent_persona</th>\n      <th>dialog</th>\n      <th>json</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[For what it's worth, I don't have a problem w...</td>\n      <td>[My apologies.  I did not have any problems wi...</td>\n      <td>2</td>\n      <td>0gleth0rpe</td>\n      <td>[for what it's worth, i don't have a problem w...</td>\n      <td>[ i did not have any problems with it, but i w...</td>\n      <td>[[For what it's worth, I don't have a problem ...</td>\n      <td>{'dialog': [['For what it's worth, I don't hav...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[Yeah I wish I could get the test some attenti...</td>\n      <td>[I took that 6 page survey and ended up where ...</td>\n      <td>1</td>\n      <td>24sparky</td>\n      <td>[]</td>\n      <td>[i took that 6 page survey and ended up where ...</td>\n      <td>[[Yeah I wish I could get the test some attent...</td>\n      <td>{'dialog': [['Yeah I wish I could get the test...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Happy New Year :)]</td>\n      <td>[Thanks for the links, im looking through them...</td>\n      <td>1</td>\n      <td>24sparky</td>\n      <td>[]</td>\n      <td>[thanks for the links, im looking through them...</td>\n      <td>[[Happy New Year :)], [Thanks for the links, i...</td>\n      <td>{'dialog': [['Happy New Year :)'], ['Thanks fo...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[Hmm, I came out as ’centrist’ but right betwe...</td>\n      <td>[World's Smallest Political Quiz:\\r\\nhttp://ww...</td>\n      <td>1</td>\n      <td>24sparky</td>\n      <td>[hmm, i came out as ’centrist’ but right betwe...</td>\n      <td>[]</td>\n      <td>[[Hmm, I came out as ’centrist’ but right betw...</td>\n      <td>{'dialog': [['Hmm, I came out as ’centrist’ bu...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[I wonder if it's more than an attitude toward...</td>\n      <td>[The problem is not guns, it's attitude toward...</td>\n      <td>8</td>\n      <td>24sparky</td>\n      <td>[i wonder if it's more than an attitude toward...</td>\n      <td>[]</td>\n      <td>[[I wonder if it's more than an attitude towar...</td>\n      <td>{'dialog': [['I wonder if it's more than an at...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3860</th>\n      <td>[Well in fact I had to flush the browser cache...</td>\n      <td>[Works fine for me]</td>\n      <td>1</td>\n      <td>zmonk2</td>\n      <td>[well in fact i had to flush the browser cache...</td>\n      <td>[]</td>\n      <td>[[Well in fact I had to flush the browser cach...</td>\n      <td>{'dialog': [['Well in fact I had to flush the ...</td>\n    </tr>\n    <tr>\n      <th>3861</th>\n      <td>[Yes, but in typical reddit style anything I p...</td>\n      <td>[What's disturbing about it?  Think it's one o...</td>\n      <td>1</td>\n      <td>zoomzoom83</td>\n      <td>[i have to complete with ron paul here.]</td>\n      <td>[ think it's one of the funniest cartoons i've...</td>\n      <td>[[Yes, but in typical reddit style anything I ...</td>\n      <td>{'dialog': [['Yes, but in typical reddit style...</td>\n    </tr>\n    <tr>\n      <th>3862</th>\n      <td>[Nope. Months of development, a day of downtim...</td>\n      <td>[ Is anything else different? Surely there's m...</td>\n      <td>4</td>\n      <td>zoomzoom83</td>\n      <td>[\\n\\n(in reality, i think it's a complete rewr...</td>\n      <td>[\\n\\nedit: i guess that sounded kinda snotty.]</td>\n      <td>[[Nope. Months of development, a day of downti...</td>\n      <td>{'dialog': [['Nope. Months of development, a d...</td>\n    </tr>\n    <tr>\n      <th>3863</th>\n      <td>[Thanks for that- I always assumed the kinetic...</td>\n      <td>[Not that it changes your point, but I think y...</td>\n      <td>4</td>\n      <td>zoomzoom83</td>\n      <td>[]</td>\n      <td>[not that it changes your point, but i think y...</td>\n      <td>[[Thanks for that- I always assumed the kineti...</td>\n      <td>{'dialog': [['Thanks for that- I always assume...</td>\n    </tr>\n    <tr>\n      <th>3864</th>\n      <td>[Yeah ok, but what I do is just go rapidly ove...</td>\n      <td>[I like it because the old system lent itself ...</td>\n      <td>1</td>\n      <td>zouhair</td>\n      <td>[and then i scroll rapidly to another thread, ...</td>\n      <td>[]</td>\n      <td>[[Yeah ok, but what I do is just go rapidly ov...</td>\n      <td>{'dialog': [['Yeah ok, but what I do is just g...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3865 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "source": [
    "list_json = []\n",
    "for column_name, item in tqdm(df_reddit_conversation.iterrows()):\n",
    "    list_json.append(create_json(item[\"dialog\"],item[\"persona\"],item[\"parent_persona\"]))\n",
    "df_reddit_conversation[\"json\"] = list_json\n",
    "df_reddit_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_conversation.to_csv(\"persona.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'dump'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-228-5af4595445b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"created_dialogues.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"wt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m   \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_json\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'dump'"
     ]
    }
   ],
   "source": [
    "with open(\"created_dialogues.json\", mode=\"wt\", encoding=\"utf-8\") as file:\n",
    "  json.dump(list_json, file, ensure_ascii=False, indent=1, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"created_dialogues.json\", \"wt\", encoding=\"utf-8\") as file:\n",
    "    for dic in list_json:\n",
    "        file.write(str(json.dumps(dic))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-247-dc6656e08729>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"created_dialogues.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "json.loads(\"created_dialogues.json\")"
   ]
  },
  {
   "source": [
    "{\"dialog\": [[\"没有 钱   万万 不行 ！ ~\"], [\"现实 就是 如此\"]], \"profile\": [{\"tag\": [\"漫画;旅遊;星座\"], \"loc\": \"广东 广州\", \"gender\": \"male\"}, {\"tag\": [\"\"], \"loc\": \"\", \"gender\": \"\"}], \"uid\": [0, 1]}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "{'dialog': [\"For what it's worth, I don't have a problem with it.\", 'My apologies.  I did not have any problems with it, but I will be more careful in the future.'], 'profile': [{'tag': [\"for what it's worth, i don't have a problem with it.\"], 'loc': '', 'gender': ''}, {'tag': [' i did not have any problems with it, but i will be more careful in the future.'], 'loc': '', 'gender': ''}], 'uid': [0, 1]}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}