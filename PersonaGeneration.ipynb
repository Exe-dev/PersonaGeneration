{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "173319109fbdc3b9e6669d3e6b9b4c0d8de214a81d3d8a11fbee43e8362bce4b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Rules of persona\r\n",
    "+ Each sentence must contain between 4 and 20 words or punctuation marks.\r\n",
    "+ It contains either the word I or my.\r\n",
    "+ At least one verb, and (iv) at least one noun, pronoun or adjective."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example Dialogue with persona\r\n",
    "- Persona: [“I like sport”, “I work a lot”]\r\n",
    "- Context: “I love running.”\r\n",
    "- Response: “Me too! But only on weekends.”"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "{\r\n",
    "    \r\n",
    "    \"dialog\": [[\"没有 钱   万万 不行 ！ ~\"], [\"现实 就是 如此\"]], \r\n",
    "    \r\n",
    "    \"profile\": [{\"tag\": [\"漫画;旅遊;星座\"], \"loc\": \"广东 广州\", \"gender\": \"male\"}, {\"tag\": [\"\"], \"loc\": \"\", \"gender\": \"\"}], \r\n",
    "    \r\n",
    "    \"uid\": [0, 1]\r\n",
    "\r\n",
    "}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "{\r\n",
    "    \r\n",
    "    \"dialog\": [[\"For what it's worth, I don't have a problem with it.\"], [\"My apologies.  I did not have any problems with it, but I will be more careful in the future.\"]], \r\n",
    "    \r\n",
    "    \"profile\": [{\"tag\": [], \"loc\": \"\", \"gender\": \"\"}, {\"tag\": [\"i did not have any problems with it, but i will be more careful in the future.\"], \r\n",
    "    \r\n",
    "    \"loc\": \"\", \"gender\": \"\"}], \r\n",
    "    \r\n",
    "    \"uid\": [0, 1]\r\n",
    "\r\n",
    "}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# test_data Example\r\n",
    "{\"uid\": [0, 1, 2], \"dialog\": [[\"剧烈运动 是 吧\"], [\"各种 剧烈运动\"], [\"... 姐 最近 有点 寂寞 过头 了 ...\"]], \"responder_profile\": {\"loc\": \"海南\", \"gender\": \"female\", \"tag\": \"美食;宅;80后\"}, \"profile\": [{\"loc\": \"天津 滨海新区\", \"gender\": \"male\", \"tag\": \"\"}, {\"loc\": \"海南\", \"gender\": \"female\", \"tag\": \"美食;宅;80后\"}, {\"loc\": \"安徽 合肥\", \"gender\": \"male\", \"tag\": \"游戏动漫;双子座;宅;音乐;90后;WOW台服众\"}], \"golden_response\": [\"可不是 ， 我 又 不 像 你 ， 有 女神 。\"]}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "import argparse\r\n",
    "import sys\r\n",
    "import pandas as pd \r\n",
    "import json\r\n",
    "import bz2\r\n",
    "from tqdm import tqdm\r\n",
    "import glob\r\n",
    "import dask.dataframe as dd\r\n",
    "from dask.diagnostics import ProgressBar\r\n",
    "import spacy\r\n",
    "import os\r\n",
    "import redditcleaner\r\n",
    "import neuralcoref"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Command Parser"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "parser = argparse.ArgumentParser(description=\"preprocess of train data\")\r\n",
    "parser.add_argument(\"--npartitions\", dest=\"npartitions\", type=int, default=10,help=\"Number of partitions\")\r\n",
    "parser.add_argument(\"--input_json\", dest=\"input_json\", type=str, default=\"./reddit_data/*/*.json\" ,help=\"Input json path\")\r\n",
    "parser.add_argument(\"--output_path\", dest=\"output_path\", type=str, default=\"./outputs\" ,help=\"Output file path\")\r\n",
    "parser.add_argument(\"--scheduler\", dest=\"scheduler\", type=str, default=\"threads\" ,help=\"Selecting Threads, Processes, or Single Threaded\")\r\n",
    "parser.add_argument(\"--is_gpu\", dest=\"is_gpu\", type=bool, default=False ,help=\"If you want to use gpu for processing dataframe, you set True\")\r\n",
    "if \"ipykernel\" in sys.modules:\r\n",
    "    args = parser.parse_args(args=[])\r\n",
    "else:\r\n",
    "    args = parser.parse_args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constant"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "NPARTITIONS = args.npartitions\r\n",
    "INPUT_JSON = args.input_json\r\n",
    "OUTPUT_PATH = args.output_path\r\n",
    "SCHEDULER = args.scheduler\r\n",
    "IS_GPU = args.is_gpu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "tqdm.pandas()\r\n",
    "ProgressBar().register()\r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "neuralcoref.add_to_pipe(nlp)\r\n",
    "\r\n",
    "#doc1 = nlp('My sister has a dog. She loves him.')\r\n",
    "#print(doc1._.coref_resolved)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1869aef58b0>"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "if(not os.path.exists(OUTPUT_PATH)):\r\n",
    "    os.makedirs(OUTPUT_PATH)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "version = len([f for f in os.listdir(OUTPUT_PATH) if \"ALL\" in f])\r\n",
    "version"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# reddit_data下の全てのjsonファイルを読み込む"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "list_bz2_file = glob.glob(INPUT_JSON)\r\n",
    "list_reddit_conversation = []\r\n",
    "list_bz2_file"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./reddit_data\\\\2007\\\\RC_2007-10.json']"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "print(\"----------read input json files----------\")\r\n",
    "for i in tqdm(range(0,len(list_bz2_file))):\r\n",
    "    with open(list_bz2_file[i], mode=\"r\", encoding=\"utf-8\") as f:\r\n",
    "        for line in f.readlines():\r\n",
    "            dic=json.loads(line)\r\n",
    "            list_reddit_conversation.append(dic)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.83it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------read input json files----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "if IS_GPU:\r\n",
    "    import cudf\r\n",
    "    df_reddit_conversation = cudf.DataFrame(list_reddit_conversation)\r\n",
    "else:\r\n",
    "    df_reddit_conversation = pd.DataFrame(list_reddit_conversation)\r\n",
    "\r\n",
    "df_reddit_conversation = df_reddit_conversation[df_reddit_conversation[\"author\"]!=\"[deleted]\"]\r\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].progress_map(lambda x:redditcleaner.clean(str(x)))\r\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].replace([\"&lt\",\"&gt\",\"&amp\"],[\"\",\"\",\"\"])\r\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].replace([\"\\\\n+\",\"\\\\r\",\"\\\\\\\\\",\"”\",\"’\"],[\"\",\"\",\"\",\"\",\"\"], regex=True)\r\n",
    "df_reddit_conversation.to_csv(f\"{OUTPUT_PATH}/AllConversation{version}.csv\")\r\n",
    "df_reddit_conversation.head(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4499/4499 [00:00<00:00, 41265.85it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    parent_id created_utc  ups  controversiality distinguished subreddit_id  \\\n",
       "0    t3_5yba3  1192450635    1                 0          None         t5_6   \n",
       "1    t3_5yba3  1192450639    2                 0          None         t5_6   \n",
       "2  t1_c02999p  1192450643    0                 0          None         t5_6   \n",
       "4  t1_c0299ah  1192450646    3                 0          None         t5_6   \n",
       "5    t3_5yba3  1192450656    1                 0          None         t5_6   \n",
       "\n",
       "        id  downs  archived   link_id  ...          author score_hidden  \\\n",
       "0  c0299an      0      True  t3_5yba3  ...         bostich        False   \n",
       "1  c0299ao      0      True  t3_5yba3  ...  igiveyoumylife        False   \n",
       "2  c0299ap      0      True  t3_5yba3  ...            Arve        False   \n",
       "4  c0299ar      0      True  t3_5yba3  ...       gigaquack        False   \n",
       "5  c0299as      0      True  t3_5yba3  ...         Percept        False   \n",
       "\n",
       "                                                body gilded  \\\n",
       "0                                               test      0   \n",
       "1  much smoother. Im just glad reddit is back, re...      0   \n",
       "2  Can we please deprecate the word \"Ajax\" now? (...      0   \n",
       "4         Oh, I see. Fancy schmancy \"submitting....\"      0   \n",
       "5                                        testing ...      0   \n",
       "\n",
       "   author_flair_text   subreddit edited author_flair_css_class        name  \\\n",
       "0               None  reddit.com  False                   None  t1_c0299an   \n",
       "1               None  reddit.com  False                   None  t1_c0299ao   \n",
       "2               None  reddit.com  False                   None  t1_c0299ap   \n",
       "4               None  reddit.com  False                   None  t1_c0299ar   \n",
       "5               None  reddit.com  False                   None  t1_c0299as   \n",
       "\n",
       "  retrieved_on  \n",
       "0   1427426409  \n",
       "1   1427426409  \n",
       "2   1427426409  \n",
       "4   1427426409  \n",
       "5   1427426409  \n",
       "\n",
       "[5 rows x 21 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>id</th>\n",
       "      <th>downs</th>\n",
       "      <th>archived</th>\n",
       "      <th>link_id</th>\n",
       "      <th>...</th>\n",
       "      <th>author</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>body</th>\n",
       "      <th>gilded</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>edited</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>name</th>\n",
       "      <th>retrieved_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>1192450635</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>c0299an</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>...</td>\n",
       "      <td>bostich</td>\n",
       "      <td>False</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>t1_c0299an</td>\n",
       "      <td>1427426409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>1192450639</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>c0299ao</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>...</td>\n",
       "      <td>igiveyoumylife</td>\n",
       "      <td>False</td>\n",
       "      <td>much smoother. Im just glad reddit is back, re...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>t1_c0299ao</td>\n",
       "      <td>1427426409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t1_c02999p</td>\n",
       "      <td>1192450643</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>c0299ap</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>...</td>\n",
       "      <td>Arve</td>\n",
       "      <td>False</td>\n",
       "      <td>Can we please deprecate the word \"Ajax\" now? (...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>t1_c0299ap</td>\n",
       "      <td>1427426409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t1_c0299ah</td>\n",
       "      <td>1192450646</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>c0299ar</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>...</td>\n",
       "      <td>gigaquack</td>\n",
       "      <td>False</td>\n",
       "      <td>Oh, I see. Fancy schmancy \"submitting....\"</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>t1_c0299ar</td>\n",
       "      <td>1427426409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>1192450656</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>c0299as</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_5yba3</td>\n",
       "      <td>...</td>\n",
       "      <td>Percept</td>\n",
       "      <td>False</td>\n",
       "      <td>testing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>t1_c0299as</td>\n",
       "      <td>1427426409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "df_reddit_conversation[df_reddit_conversation[\"body\"].str.contains(\"There's something called an\")]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       parent_id created_utc  ups  controversiality distinguished  \\\n",
       "6107  t1_c029beg  1192491915    2                 0          None   \n",
       "\n",
       "     subreddit_id       id  downs  archived   link_id  ...  author  \\\n",
       "6107         t5_6  c029e0e      0      True  t3_5ybod  ...  Aegeus   \n",
       "\n",
       "     score_hidden                                               body gilded  \\\n",
       "6107        False  There's something called an \"edit button\". It ...      0   \n",
       "\n",
       "      author_flair_text   subreddit edited author_flair_css_class        name  \\\n",
       "6107               None  reddit.com  False                   None  t1_c029e0e   \n",
       "\n",
       "     retrieved_on  \n",
       "6107   1427426349  \n",
       "\n",
       "[1 rows x 21 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>id</th>\n",
       "      <th>downs</th>\n",
       "      <th>archived</th>\n",
       "      <th>link_id</th>\n",
       "      <th>...</th>\n",
       "      <th>author</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>body</th>\n",
       "      <th>gilded</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>edited</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>name</th>\n",
       "      <th>retrieved_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6107</th>\n",
       "      <td>t1_c029beg</td>\n",
       "      <td>1192491915</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>c029e0e</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_5ybod</td>\n",
       "      <td>...</td>\n",
       "      <td>Aegeus</td>\n",
       "      <td>False</td>\n",
       "      <td>There's something called an \"edit button\". It ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>t1_c029e0e</td>\n",
       "      <td>1427426349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 会話ペアの作成"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "df_reddit_conversation[\"removed_prefix_parent_id\"] = df_reddit_conversation[\"parent_id\"].str.replace(\"t\\d_\",\"\")\r\n",
    "df_reddit_conversation[\"parent_body\"] = df_reddit_conversation[df_reddit_conversation[\"removed_prefix_parent_id\"]==df_reddit_conversation[\"id\"]][\"body\"]\r\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].str.replace('\\\"','’')\r\n",
    "df_reddit_conversation[\"parent_body\"] = df_reddit_conversation[\"parent_body\"].str.replace('\\\"','’')\r\n",
    "df_reddit_conversation = pd.merge(df_reddit_conversation,df_reddit_conversation[[\"id\",\"body\"]].rename(columns={\"id\":\"parent_id\",\"body\":\"parent_body\"}),left_on=\"removed_prefix_parent_id\",right_on=\"parent_id\").drop(columns=[\"parent_body_x\",\"parent_id_y\"]).rename(columns={\"parent_body_y\":\"parent_body\"})\r\n",
    "df_reddit_conversation = df_reddit_conversation.dropna(subset=[\"parent_body\"]).sort_values([\"author\"]).reset_index(drop=True)\r\n",
    "df_reddit_conversation[\"original_body\"] = df_reddit_conversation[\"body\"]\r\n",
    "df_reddit_conversation[\"original_parent_body\"] = df_reddit_conversation[\"parent_body\"]\r\n",
    "df_reddit_conversation = df_reddit_conversation[[\"body\",\"parent_body\",\"original_body\",\"original_parent_body\",\"ups\",\"author\"]]\r\n",
    "df_reddit_conversation"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-123-15e556bf2943>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit_conversation[\"removed_prefix_parent_id\"] = df_reddit_conversation[\"parent_id\"].str.replace(\"t\\d_\",\"\")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   body  \\\n",
       "0     The real truth is...they're 25. They don't thi...   \n",
       "1                         Actually, yes. Read the bill.   \n",
       "2     That's not a biased report at all....not at al...   \n",
       "3     Right. But if you siphon off funds to cover ad...   \n",
       "4     Because the money that comes to my state is mi...   \n",
       "...                                                 ...   \n",
       "1827  This is indeed a great example of econ theory ...   \n",
       "1828  What shiner_man is trying to say is Al Gore hi...   \n",
       "1829  Done that, before writing the above.. there's ...   \n",
       "1830  Nope. Months of development, a day of downtime...   \n",
       "1831                                                 No   \n",
       "\n",
       "                                            parent_body  \\\n",
       "0     How do you know they are lazy? Prove to me tha...   \n",
       "1                                         Actually, no.   \n",
       "2     Actually no, read the facts. http://www.famili...   \n",
       "3     Of course, but the program overwhelmingly bene...   \n",
       "4     And...what exactly is wrong with this? If you ...   \n",
       "...                                                 ...   \n",
       "1827  An economist walks up to you and a friend and ...   \n",
       "1828  Which ignores the fact that millions of left-w...   \n",
       "1829  Firefox: Tools  Options  Privacy  Show Cookies...   \n",
       "1830  Is anything else different? Surely there's mor...   \n",
       "1831  Can we please deprecate the word ’Ajax’ now? (...   \n",
       "\n",
       "                                          original_body  \\\n",
       "0     The real truth is...they're 25. They don't thi...   \n",
       "1                         Actually, yes. Read the bill.   \n",
       "2     That's not a biased report at all....not at al...   \n",
       "3     Right. But if you siphon off funds to cover ad...   \n",
       "4     Because the money that comes to my state is mi...   \n",
       "...                                                 ...   \n",
       "1827  This is indeed a great example of econ theory ...   \n",
       "1828  What shiner_man is trying to say is Al Gore hi...   \n",
       "1829  Done that, before writing the above.. there's ...   \n",
       "1830  Nope. Months of development, a day of downtime...   \n",
       "1831                                                 No   \n",
       "\n",
       "                                   original_parent_body  ups       author  \n",
       "0     How do you know they are lazy? Prove to me tha...    1  60MinuteMan  \n",
       "1                                         Actually, no.   -1  60MinuteMan  \n",
       "2     Actually no, read the facts. http://www.famili...    0  60MinuteMan  \n",
       "3     Of course, but the program overwhelmingly bene...    1  60MinuteMan  \n",
       "4     And...what exactly is wrong with this? If you ...    1  60MinuteMan  \n",
       "...                                                 ...  ...          ...  \n",
       "1827  An economist walks up to you and a friend and ...    5       zipdog  \n",
       "1828  Which ignores the fact that millions of left-w...    3   zmigliozzi  \n",
       "1829  Firefox: Tools  Options  Privacy  Show Cookies...    2         zoli  \n",
       "1830  Is anything else different? Surely there's mor...    4   zoomzoom83  \n",
       "1831  Can we please deprecate the word ’Ajax’ now? (...    2   zoomzoom83  \n",
       "\n",
       "[1832 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>parent_body</th>\n",
       "      <th>original_body</th>\n",
       "      <th>original_parent_body</th>\n",
       "      <th>ups</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The real truth is...they're 25. They don't thi...</td>\n",
       "      <td>How do you know they are lazy? Prove to me tha...</td>\n",
       "      <td>The real truth is...they're 25. They don't thi...</td>\n",
       "      <td>How do you know they are lazy? Prove to me tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>60MinuteMan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actually, yes. Read the bill.</td>\n",
       "      <td>Actually, no.</td>\n",
       "      <td>Actually, yes. Read the bill.</td>\n",
       "      <td>Actually, no.</td>\n",
       "      <td>-1</td>\n",
       "      <td>60MinuteMan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That's not a biased report at all....not at al...</td>\n",
       "      <td>Actually no, read the facts. http://www.famili...</td>\n",
       "      <td>That's not a biased report at all....not at al...</td>\n",
       "      <td>Actually no, read the facts. http://www.famili...</td>\n",
       "      <td>0</td>\n",
       "      <td>60MinuteMan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Right. But if you siphon off funds to cover ad...</td>\n",
       "      <td>Of course, but the program overwhelmingly bene...</td>\n",
       "      <td>Right. But if you siphon off funds to cover ad...</td>\n",
       "      <td>Of course, but the program overwhelmingly bene...</td>\n",
       "      <td>1</td>\n",
       "      <td>60MinuteMan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Because the money that comes to my state is mi...</td>\n",
       "      <td>And...what exactly is wrong with this? If you ...</td>\n",
       "      <td>Because the money that comes to my state is mi...</td>\n",
       "      <td>And...what exactly is wrong with this? If you ...</td>\n",
       "      <td>1</td>\n",
       "      <td>60MinuteMan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>This is indeed a great example of econ theory ...</td>\n",
       "      <td>An economist walks up to you and a friend and ...</td>\n",
       "      <td>This is indeed a great example of econ theory ...</td>\n",
       "      <td>An economist walks up to you and a friend and ...</td>\n",
       "      <td>5</td>\n",
       "      <td>zipdog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>What shiner_man is trying to say is Al Gore hi...</td>\n",
       "      <td>Which ignores the fact that millions of left-w...</td>\n",
       "      <td>What shiner_man is trying to say is Al Gore hi...</td>\n",
       "      <td>Which ignores the fact that millions of left-w...</td>\n",
       "      <td>3</td>\n",
       "      <td>zmigliozzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>Done that, before writing the above.. there's ...</td>\n",
       "      <td>Firefox: Tools  Options  Privacy  Show Cookies...</td>\n",
       "      <td>Done that, before writing the above.. there's ...</td>\n",
       "      <td>Firefox: Tools  Options  Privacy  Show Cookies...</td>\n",
       "      <td>2</td>\n",
       "      <td>zoli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>Nope. Months of development, a day of downtime...</td>\n",
       "      <td>Is anything else different? Surely there's mor...</td>\n",
       "      <td>Nope. Months of development, a day of downtime...</td>\n",
       "      <td>Is anything else different? Surely there's mor...</td>\n",
       "      <td>4</td>\n",
       "      <td>zoomzoom83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>No</td>\n",
       "      <td>Can we please deprecate the word ’Ajax’ now? (...</td>\n",
       "      <td>No</td>\n",
       "      <td>Can we please deprecate the word ’Ajax’ now? (...</td>\n",
       "      <td>2</td>\n",
       "      <td>zoomzoom83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1832 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "def CreatePersona(body: str):\r\n",
    "    doc = nlp(body.lower())\r\n",
    "    # 文ごとに分割\r\n",
    "    persona = [str(sentence) for sentence in doc.sents if IsPersona(str(sentence))]\r\n",
    "    return persona"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "def IsPersona(sentence: str):\r\n",
    "    # 以下の3つの条件を満たすものをペルソナとする\r\n",
    "    # 1.文の単語数が4-20の間\r\n",
    "    # 2.I か my　が含まれている\r\n",
    "    # 3.少なくとも1つの動詞と，名詞，代名詞，形容詞のいずれかが含まれている\r\n",
    "    words = [str(word) for word in nlp(sentence.strip())]\r\n",
    "    poses = [token.pos_ for token in nlp(sentence.strip())]\r\n",
    "    return (\r\n",
    "        (4 <= len(words) <= 20)&\r\n",
    "        (not set([\"i\",\"my\"]).isdisjoint(set(words)))&\r\n",
    "        ((\"VERB\" in poses)&(not set([\"NOUN\", \"ADJ\", \"PROPN\"]).isdisjoint(set(poses))))\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "def create_json(row):\r\n",
    "    return {\r\n",
    "        \"dialog\":row[\"dialog\"],\r\n",
    "        \"profile\":[\r\n",
    "            {\"tag\":row[\"persona\"],\r\n",
    "            \"loc\":\"\",\r\n",
    "            \"gender\":\"\"},\r\n",
    "            {\"tag\":row[\"parent_persona\"],\r\n",
    "            \"loc\":\"\",\r\n",
    "            \"gender\":\"\"}\r\n",
    "        ],\r\n",
    "        \"uid\":[0,1]\r\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ペルソナの作成"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "print(\"----------create conversation pair ----------\")\r\n",
    "if not IS_GPU:\r\n",
    "    ddf_reddit_conversation = dd.from_pandas(data=df_reddit_conversation, npartitions=NPARTITIONS)\r\n",
    "    ddf_reddit_conversation[\"persona\"] = ddf_reddit_conversation[\"original_body\"].map(CreatePersona)\r\n",
    "    ddf_reddit_conversation[\"parent_persona\"] = ddf_reddit_conversation[\"original_parent_body\"].map(CreatePersona)\r\n",
    "    ddf_reddit_conversation.query(\"persona.notnull() | parent_persona.notnull()\")\r\n",
    "    ddf_reddit_conversation[\"body\"] = ddf_reddit_conversation[\"body\"].map(lambda sentence:nlp(sentence)._.coref_resolved)\r\n",
    "    ddf_reddit_conversation[\"parent_body\"] = ddf_reddit_conversation[\"parent_body\"].map(lambda sentence:nlp(sentence)._.coref_resolved)\r\n",
    "    df_reddit_conversation = ddf_reddit_conversation.compute(scheduler=SCHEDULER)\r\n",
    "else:\r\n",
    "    df_reddit_conversation[\"persona\"] = df_reddit_conversation[\"original_body\"].map(CreatePersona)\r\n",
    "    df_reddit_conversation[\"parent_persona\"] = df_reddit_conversation[\"original_parent_body\"].map(CreatePersona)\r\n",
    "    df_reddit_conversation.query(\"persona.notnull() | parent_persona.notnull()\")\r\n",
    "    df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].map(lambda sentence:nlp(sentence)._.coref_resolved)\r\n",
    "    df_reddit_conversation[\"parent_body\"] = df_reddit_conversation[\"parent_body\"].map(lambda sentence:nlp(sentence)._.coref_resolved)\r\n",
    "df_reddit_conversation"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------create conversation pair ----------\n",
      "[#############                           ] | 33% Completed |  2min  0.2s"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"--------- create list ----------\")\r\n",
    "df_reddit_conversation[\"body\"] = df_reddit_conversation[\"body\"].progress_map(lambda x: [x] )\r\n",
    "df_reddit_conversation[\"parent_body\"] = df_reddit_conversation[\"parent_body\"].progress_map(lambda x: [x] )\r\n",
    "df_reddit_conversation[\"dialog\"] = [list(x) for x in zip(df_reddit_conversation[\"body\"].tolist(),df_reddit_conversation[\"parent_body\"].tolist())]\r\n",
    "df_reddit_conversation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Json形式の作成"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_reddit_conversation[\"json\"] = df_reddit_conversation.progress_apply(create_json, axis=1)\r\n",
    "df_reddit_conversation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Outputs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_reddit_conversation.to_csv(f\"{OUTPUT_PATH}/persona{version}.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list_json = df_reddit_conversation[\"json\"].tolist()\r\n",
    "with open(f\"{OUTPUT_PATH}/created_dialogues{version}.json\", \"wt\", encoding=\"utf-8\") as file:\r\n",
    "    for dic in list_json:\r\n",
    "        file.write(str(json.dumps(dic))+\"\\n\")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import subprocess\r\n",
    "subprocess.run(['jupyter', 'nbconvert', '--to', 'script', '*.ipynb'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CompletedProcess(args=['jupyter', 'nbconvert', '--to', 'script', '*.ipynb'], returncode=0)"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  }
 ]
}